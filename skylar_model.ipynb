{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sky1113/CIS4190/blob/main/skylar_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0bKqI_UZNso"
   },
   "source": [
    "# Installing dependencies\n",
    "\n",
    "Please make a copy of this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBj9R3VCAYkv"
   },
   "outputs": [],
   "source": [
    "!pip install geopy > delete.txt\n",
    "!pip install datasets > delete.txt\n",
    "!pip install torch torchvision datasets > delete.txt\n",
    "!pip install huggingface_hub > delete.txt\n",
    "!pip install transformers > delete.txt\n",
    "!rm delete.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_d8uxEsCZTBc"
   },
   "source": [
    "# Huggingface login\n",
    "You will require your personal token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5vOXHY5Ifv2"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sv_nG_P8W7Bn"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GokzmxKZupF"
   },
   "source": [
    "## Downloading the train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i80m6Kr6I_8A"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Image\n",
    "\n",
    "dataset_train = load_dataset(\"CIS-5190-Final-Project/Images\", split=\"train\")\n",
    "dataset_test = load_dataset(\"CIS-5190-Final-Project/Images\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PREtoGsKCWx"
   },
   "outputs": [],
   "source": [
    "print(dataset_train)\n",
    "print(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cB4MQym2Z3OB"
   },
   "source": [
    "## Defining the Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtBHXyCJgO2r"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class CustomResNetModel(nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(self, model_name=\"microsoft/resnet-18\", num_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load pre-trained ResNet model from Hugging Face\n",
    "        self.resnet = AutoModelForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "        # Access the Linear layer within the Sequential classifier\n",
    "        in_features = self.resnet.classifier[1].in_features  # Accessing the Linear layer within the Sequential\n",
    "\n",
    "        # Modify the classifier layer to have the desired number of output classes\n",
    "        self.resnet.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "    def save_model(self, save_path):\n",
    "        \"\"\"Save model locally using the Hugging Face format.\"\"\"\n",
    "        self.save_pretrained(save_path)\n",
    "\n",
    "    def push_model(self, repo_name):\n",
    "        \"\"\"Push the model to the Hugging Face Hub.\"\"\"\n",
    "        self.push_to_hub(repo_name)\n",
    "\n",
    "class GPSImageDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None, lat_mean=None, lat_std=None, lon_mean=None, lon_std=None):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "        # Compute mean and std from the dataframe if not provided\n",
    "        self.latitude_mean = lat_mean if lat_mean is not None else np.mean(np.array(self.hf_dataset['Latitude']))\n",
    "        self.latitude_std = lat_std if lat_std is not None else np.std(np.array(self.hf_dataset['Latitude']))\n",
    "        self.longitude_mean = lon_mean if lon_mean is not None else np.mean(np.array(self.hf_dataset['Longitude']))\n",
    "        self.longitude_std = lon_std if lon_std is not None else np.std(np.array(self.hf_dataset['Longitude']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract data\n",
    "        example = self.hf_dataset[idx]\n",
    "\n",
    "        # Load and process the image\n",
    "        image = example['image']\n",
    "        latitude = example['Latitude']\n",
    "        longitude = example['Longitude']\n",
    "        # image = image.rotate(-90, expand=True)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Normalize GPS coordinates\n",
    "        latitude = (latitude - self.latitude_mean) / self.latitude_std\n",
    "        longitude = (longitude - self.longitude_mean) / self.longitude_std\n",
    "        gps_coords = torch.tensor([latitude, longitude], dtype=torch.float32)\n",
    "\n",
    "        return image, gps_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ua-UFAGyaFvL"
   },
   "source": [
    "## Creating dataloaders and visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4BkKWQMQ8Co"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),  # Random crop and resize to 224x224\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "    transforms.RandomRotation(degrees=15),  # Random rotation between -15 and 15 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color jitter\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Optionally, you can create a separate transform for inference without augmentations\n",
    "inference_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Create the training dataset and dataloader\n",
    "train_dataset = GPSImageDataset(hf_dataset=dataset_train, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Retrieve normalization parameters from the training dataset\n",
    "lat_mean = train_dataset.latitude_mean\n",
    "lat_std = train_dataset.latitude_std\n",
    "lon_mean = train_dataset.longitude_mean\n",
    "lon_std = train_dataset.longitude_std\n",
    "\n",
    "# Create the validation dataset and dataloader using training mean and std\n",
    "val_dataset = GPSImageDataset(\n",
    "    hf_dataset=dataset_test,\n",
    "    transform=inference_transform,\n",
    "    lat_mean=lat_mean,\n",
    "    lat_std=lat_std,\n",
    "    lon_mean=lon_mean,\n",
    "    lon_std=lon_std\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i028XRKnh6-v"
   },
   "outputs": [],
   "source": [
    "# Verify loading\n",
    "for images, gps_coords in train_dataloader:\n",
    "    print(images.size(), gps_coords.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkaQtacFiWK8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "def denormalize(tensor, mean, std):\n",
    "    mean = np.array(mean)\n",
    "    std = np.array(std)\n",
    "    tensor = tensor.numpy().transpose((1, 2, 0))  # Convert from C x H x W to H x W x C\n",
    "    tensor = std * tensor + mean  # Denormalize\n",
    "    tensor = np.clip(tensor, 0, 1)  # Clip to keep pixel values between 0 and 1\n",
    "    return tensor\n",
    "\n",
    "data_iter = iter(train_dataloader)\n",
    "images, gps_coords = next(data_iter)  # Get a batch of images and labels\n",
    "# Denormalize the first image in the batch for display\n",
    "itr = 0\n",
    "for im in images:\n",
    "  image = denormalize(im, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "  # Plot the image\n",
    "  plt.imshow(image)\n",
    "  plt.title(f'Latitude: {gps_coords[itr][0].item():.4f}, Longitude: {gps_coords[itr][1].item():.4f}')\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "  itr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYriL36vW3eM"
   },
   "source": [
    "# Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9WXBRBJlJ57"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from geopy.distance import geodesic\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "# Load the pre-trained ResNet18 model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the last fully connected layer to output 2 values (latitude and longitude)\n",
    "num_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_features, 2)  # Latitude and Longitude\n",
    "\n",
    "# Not freeze pre-trained weights (excluding the final layer)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in resnet.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(resnet.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "# Use ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# Training loop with validation\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    resnet.train()\n",
    "    running_loss = 0.0\n",
    "    for images, gps_coords in train_dataloader:\n",
    "        images, gps_coords = images.to(device), gps_coords.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = resnet(images)\n",
    "        loss = criterion(outputs, gps_coords)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    resnet.eval()\n",
    "    val_loss = 0.0\n",
    "    baseline_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, gps_coords in val_dataloader:\n",
    "            images = images.to(device)\n",
    "            gps_coords = gps_coords.to(device)\n",
    "\n",
    "            batch_size = gps_coords.size(0)\n",
    "            total_samples += batch_size\n",
    "\n",
    "            # Model predictions\n",
    "            outputs = resnet(images)\n",
    "\n",
    "            # Denormalize predictions and actual GPS coordinates\n",
    "            preds_denorm = outputs.cpu().numpy() * np.array([lat_std, lon_std]) + np.array([lat_mean, lon_mean])\n",
    "            actuals_denorm = gps_coords.cpu().numpy() * np.array([lat_std, lon_std]) + np.array([lat_mean, lon_mean])\n",
    "\n",
    "            # Compute geodesic distances between predictions and actuals\n",
    "            for pred, actual in zip(preds_denorm, actuals_denorm):\n",
    "                distance = geodesic((actual[0], actual[1]), (pred[0], pred[1])).meters\n",
    "                val_loss += distance ** 2  # Squared distance\n",
    "\n",
    "            # Baseline predictions\n",
    "            baseline_preds = np.array([lat_mean, lon_mean])\n",
    "\n",
    "            # Compute geodesic distances between baseline preds and actuals\n",
    "            for actual in actuals_denorm:\n",
    "                distance = geodesic((actual[0], actual[1]), (baseline_preds[0], baseline_preds[1])).meters\n",
    "                baseline_loss += distance ** 2  # Squared distance\n",
    "\n",
    "    # Compute average losses\n",
    "    val_loss /= total_samples\n",
    "    baseline_loss /= total_samples\n",
    "\n",
    "    # Compute RMSE\n",
    "    val_rmse = np.sqrt(val_loss)\n",
    "    baseline_rmse = np.sqrt(baseline_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Loss (meters^2): {val_loss:.2f}, Baseline Loss (meters^2): {baseline_loss:.2f}\")\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation RMSE (meters): {val_rmse:.2f}, Baseline RMSE (meters): {baseline_rmse:.2f}\")\n",
    "\n",
    "    # Update the scheduler with the validation metric (choose one - val_loss or val_rmse)\n",
    "    scheduler.step(val_rmse)  # Using val_loss as the metric to reduce learning rate\n",
    "\n",
    "print('Training complete')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(resnet.state_dict(), 'resnet_gps_regressor.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdLEWIKhaX_o"
   },
   "source": [
    "# Testing the learnt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "6AdRxPfkWy2n",
    "outputId": "8adebe6b-6a7b-4645-8e6d-75d1f6302201"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Initialize lists to store predictions and actual values\n",
    "all_preds = []\n",
    "all_actuals = []\n",
    "\n",
    "resnet.eval()\n",
    "with torch.no_grad():\n",
    "    for images, gps_coords in val_dataloader:\n",
    "        images, gps_coords = images.to(device), gps_coords.to(device)\n",
    "\n",
    "        outputs = resnet(images)\n",
    "\n",
    "        # Denormalize predictions and actual values\n",
    "        preds = outputs.cpu() * torch.tensor([lat_std, lon_std]) + torch.tensor([lat_mean, lon_mean])\n",
    "        actuals = gps_coords.cpu() * torch.tensor([lat_std, lon_std]) + torch.tensor([lat_mean, lon_mean])\n",
    "\n",
    "        all_preds.append(preds)\n",
    "        all_actuals.append(actuals)\n",
    "\n",
    "# Concatenate all batches\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_actuals = torch.cat(all_actuals).numpy()\n",
    "\n",
    "# Compute error metrics\n",
    "mae = mean_absolute_error(all_actuals, all_preds)\n",
    "rmse = mean_squared_error(all_actuals, all_preds, squared=False)\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Root Mean Squared Error: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5dBFdULYk0v"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Retrieve normalization parameters from the training dataset\n",
    "lat_mean = train_dataset.latitude_mean\n",
    "lat_std = train_dataset.latitude_std\n",
    "lon_mean = train_dataset.longitude_mean\n",
    "lon_std = train_dataset.longitude_std\n",
    "\n",
    "# Denormalize predictions and actual values\n",
    "all_preds_denorm = all_preds * np.array([lat_std, lon_std]) + np.array([lat_mean, lon_mean])\n",
    "all_actuals_denorm = all_actuals * np.array([lat_std, lon_std]) + np.array([lat_mean, lon_mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "12BMgT0CXVu7",
    "outputId": "b2a3cfb5-2117-4e21-ca52-ee1255295210"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot actual points\n",
    "plt.scatter(all_actuals_denorm[:, 1], all_actuals_denorm[:, 0], label='Actual', color='blue', alpha=0.6)\n",
    "\n",
    "# Plot predicted points\n",
    "plt.scatter(all_preds_denorm[:, 1], all_preds_denorm[:, 0], label='Predicted', color='red', alpha=0.6)\n",
    "\n",
    "# Draw lines connecting actual and predicted points\n",
    "for i in range(len(all_actuals_denorm)):\n",
    "    plt.plot(\n",
    "        [all_actuals_denorm[i, 1], all_preds_denorm[i, 1]],\n",
    "        [all_actuals_denorm[i, 0], all_preds_denorm[i, 0]],\n",
    "        color='gray', linewidth=0.5\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Actual vs. Predicted GPS Coordinates with Error Lines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcG68M_4hqu4"
   },
   "source": [
    "# 1. Pushing the Modle to the Hugging Face(HF Model)\n",
    "\n",
    "Use this code if you loaded model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyzw7E30hrw8"
   },
   "outputs": [],
   "source": [
    "resnet.push_to_hub(\"CIS-5190-Final-Project/model_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPH15fyngNl7"
   },
   "source": [
    "You load this model by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npO5ujIYhAK4"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, Repository, snapshot_download\n",
    "\n",
    "# Initialize HfApi\n",
    "api = HfApi()\n",
    "\n",
    "# modify repo_name if necessary\n",
    "repo_name = \"CIS-5190-Final-Project/model_v2\" # Adjust repo name as needed\n",
    "\n",
    "# Create or get the repository\n",
    "repo_url = api.create_repo(repo_id=repo_name, exist_ok=True)\n",
    "\n",
    "# Save the model locally first\n",
    "torch.save(resnet, \"resnet_gps_regressor.pth\")  # Or your desired local path\n",
    "\n",
    "# Upload the model to the Hub\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"resnet_gps_regressor.pth\",\n",
    "    path_in_repo=\"resnet_gps_regressor.pth\",  # You can change the filename on the Hub\n",
    "    repo_id=repo_name,\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqEp8OeygSgh"
   },
   "outputs": [],
   "source": [
    "model=resnet.from_pretrained(\"CIS-5190-Final-Project/model_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-nfMsIf0MHd"
   },
   "source": [
    "# 2. Pushing the model to the Hugging Face (Torchvision Model)\n",
    "\n",
    "Use this code if you loaded the model from Torchvision or built the model from scratch using PyTorch. If you built your model from scratch, make sure to follow the guidelines described here - https://huggingface.co/docs/hub/en/models-uploading#upload-a-pytorch-model-using-huggingfacehub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1i22h251R-e"
   },
   "outputs": [],
   "source": [
    "path_name = \"resnet_gps_regressor_complete.pth\"\n",
    "model_save_path = \"/resnet_gps_regressor_complete.pth\"\n",
    "torch.save(resnet, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Pw_R-cF1ull"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, Repository\n",
    "\n",
    "# Initialize HfApi\n",
    "api = HfApi()\n",
    "\n",
    "# modify repo_name if necessary\n",
    "repo_name = \"ImageToGPSproject_resnet18_v2\"\n",
    "organization_name = \"CIS-5190-Final-Project\"\n",
    "repo_url = api.create_repo(repo_id=f\"{organization_name}/{repo_name}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install git-lfs\n",
    "\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9A6ADyb2pSy"
   },
   "outputs": [],
   "source": [
    "repo_local_dir = \"/ImageToGPSproject_resnet18_v2\"\n",
    "repo = Repository(local_dir=repo_local_dir, clone_from=repo_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-S4-AGNU3GIw"
   },
   "outputs": [],
   "source": [
    "os.rename(model_save_path, f\"{repo_local_dir}/resnet_gps_regressor_complete.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83AQht153Moa"
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"srearick@seas.upenn.edu\"\n",
    "!git config --global user.name \"srearick\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yF5ovcpG3STu"
   },
   "outputs": [],
   "source": [
    "repo.push_to_hub(commit_message=\"Add fine-tuned ResNet18 model for Image to GPS project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFT5lm-1gtiF"
   },
   "source": [
    "You load this model by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zgq3Z9zngu6K"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "# Specify the repository and the filename of the model you want to load\n",
    "repo_id = \"{organization_name}/{repo_name}\"  # Replace with your repo name\n",
    "filename = \"{path_name}\"\n",
    "\n",
    "model_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "\n",
    "# Load the model using torch\n",
    "model_test = torch.load(model_path)\n",
    "model_test.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lat_mean)\n",
    "print(lat_std)\n",
    "print(lon_mean)\n",
    "print(lon_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
